##
## common settings for all apps
##

## NOTE - README table was generated with https://github.com/norwoodj/helm-docs

nameOverride: ""
fullNameOverride: ""

## LONG_RANDOM_STRING -- A long random string. You should probably provide your own. This is needed for OIDC.
##
LONG_RANDOM_STRING: 1234

nodeSelector: {}
affinity: {}
tolerations: []
podAnnotations: {}

##
## Configuration related to the search service.
##
search:
  ## search.serviceName -- The search service name.
  serviceName: search

  ## search.serviceType -- The search service type. See service types [ref](https://kubernetes.io/docs/concepts/services-networking/service/#publishing-services-service-types)
  serviceType: ClusterIP

  ## search.elasticsearchEndpoint -- The name of the service hosting elasticsearch on your cluster, if you bring your own. You should only need to change this, if you don't use the version in this chart.
  elasticsearchEndpoint:

  ## search.image -- The image of the search container.
  image: amundsendev/amundsen-search

  ## search.imageTag -- The image tag of the search container.
  imageTag: 2.4.0

  ## search.replicas -- How many replicas of the search service to run.
  replicas: 1

  ## search.resources -- See pod resourcing [ref](https://kubernetes.io/docs/concepts/configuration/manage-compute-resources-container/)
  resources: {}
  #  limits:
  #    cpu: 2
  #    memory: 2Gi
  #  requests:
  #    cpu: 1
  #    memory: 1Gi

  nodeSelector: {}
  affinity: {}
  tolerations: []
  annotations: {}
  podAnnotations: {}

##
## Configuration related to the metadata service.
##
metadata:
  ## metadata.serviceName -- The metadata service name.
  serviceName: metadata

  ## metadata.serviceType -- The metadata service type. See service types [ref](https://kubernetes.io/docs/concepts/services-networking/service/#publishing-services-service-types)
  serviceType: ClusterIP

  ## metadata.neo4jEndpoint -- The name of the service hosting neo4j on your cluster, if you bring your own. You should only need to change this, if you don't use the version in this chart.
  neo4jEndpoint:

  ## metadata.image -- The image of the metadata container.
  image: amundsendev/amundsen-metadata

  ## metadata.imageTag -- The image tag of the metadata container.
  imageTag: 2.5.4

  ## metadata.replicas -- How many replicas of the metadata service to run.
  replicas: 1

  ## metadata.resources -- See pod resourcing [ref](https://kubernetes.io/docs/concepts/configuration/manage-compute-resources-container/)
  resources: {}
  #  limits:
  #    cpu: 2
  #    memory: 2Gi
  #  requests:
  #    cpu: 1
  #    memory: 1Gi

  nodeSelector: {}
  affinity: {}
  tolerations: []
  annotations: {}
  podAnnotations: {}

##
## Configuration related to the frontEnd service.
##
frontEnd:
  ## frontEnd.serviceName -- The frontend service name.
  serviceName: frontend

  ## frontEnd.serviceType -- The frontend service type. See service types [ref](https://kubernetes.io/docs/concepts/services-networking/service/#publishing-services-service-types)
  serviceType: ClusterIP

  ## frontEnd.image -- The image of the frontend container.
  image: amundsendev/amundsen-frontend

  ## frontEnd.imageTag -- The image tag of the frontend container.
  imageTag: 2.1.1

  ## frontEnd.servicePort -- The port the frontend service will be exposed on via the loadbalancer.
  servicePort: 80

  ## frontEnd.replicas -- How many replicas of the frontend service to run.
  replicas: 1

  ## frontEnd.baseUrl -- used by notifications util to provide links to amundsen pages in emails.
  baseUrl: http://localhost

  ## frontEnd.oidcEnabled -- To enable auth via OIDC, set this to true.
  oidcEnabled: false

  ## frontEnd.createOidcSecret -- OIDC needs some configuration. If you want the chart to make your secrets, set this to true and set the next four values. If you don't want to configure your secrets via helm, you can still use the amundsen-oidc-config.yaml as a template
  createOidcSecret: false

  ## frontEnd.OIDC_CLIENT_ID -- The client id for OIDC.
  OIDC_CLIENT_ID:
  ## frontEnd.OIDC_CLIENT_SECRET -- The client secret for OIDC.
  OIDC_CLIENT_SECRET: ""
  ## frontEnd.OIDC_ORG_URL -- The organization URL for OIDC.
  OIDC_ORG_URL:
  ## frontEnd.OIDC_AUTH_SERVER_ID -- The authorization server id for OIDC.
  OIDC_AUTH_SERVER_ID:

  ## frontEnd.resources -- See pod resourcing [ref](https://kubernetes.io/docs/concepts/configuration/manage-compute-resources-container/)
  resources: {}
  #  limits:
  #    cpu: 2
  #    memory: 2Gi
  #  requests:
  #    cpu: 1
  #    memory: 1Gi

  nodeSelector: {}
  affinity: {}
  tolerations: []
  annotations: {}
  podAnnotations: {}

  ingress:
    enabled: false
    annotations: {}
    # kubernetes.io/ingress.class: nginx
    # kubernetes.io/tls-acme: "true"
    path: /
    hosts:
        - frontend.example.com
    tls: []
    #  - secretName:frontend.example.com
    #    hosts:
    #      - frontend.example.com

##
## Configuration related to neo4j.
## https://github.com/neo4j-contrib/neo4j-helm/blob/master/values.yaml
##
neo4j:
  # neo4j.enabled
  enabled: true

  # Use password authentication
  authEnabled: true

  ## Specify password for neo4j user
  ## Defaults to a random 10-character alphanumeric string if not set and authEnabled is true
  # neo4jPassword:

  # neo4j.resources
  resources: {}
  #  limits:
  #    cpu: 2
  #    memory: 2Gi
  # requests:
  #    cpu: 1
  #    memory: 1Gi

  # Cores
  core:
    # configMap: "my-custom-configmap"
    standalone: true
    persistentVolume:
      ## whether or not persistence is enabled
      ##
      enabled: true

  dbms:
    memory:
      use_memrec: false
      heap:
        initial_size: ""
        max_size: ""
      pagecache:
        size: ""


##
## Configuration related to elasticsearch.
##
## To add values to dependent charts, prefix the value with the chart name (e.g. elasticsearch)
## By default, the ES chart runs with 3,3,2 nodes for master, data, client. Amundsen likely does not need so much,
## so, this has been tuned down to 1,1,1.
##
elasticsearch:
  # elasticsearch.enabled -- set this to false, if you want to provide your own ES instance.
  enabled: true
  cluster:
    env:
      ## elasticsearch.cluster.env.MINIMUM_MASTER_NODES -- required to match master.replicas
      MINIMUM_MASTER_NODES: 1
      ## elasticsearch.cluster.env.EXPECTED_MASTER_NODES -- required to match master.replicas
      EXPECTED_MASTER_NODES: 1
      ## elasticsearch.cluster.env.RECOVER_AFTER_MASTER_NODES -- required to match master.replicas
      RECOVER_AFTER_MASTER_NODES: 1
  master:
    ## elasticsearch.master.replicas -- only running amundsen on 1 master replica
    replicas: 1
  data:
    ## elasticsearch.data.replicas -- only running amundsen on 1 data replica
    replicas: 1
  client:
    ## elasticsearch.client.replicas -- only running amundsen on 1 client replica
    replicas: 1
  #  serviceType: LoadBalancer
  #  serviceAnnotations:
  #    external-dns.alpha.kubernetes.io/hostname: amundsen-elasticsearch.dev.teamname.company.com
  #    service.beta.kubernetes.io/aws-load-balancer-internal: 0.0.0.0/0
  #    service.beta.kubernetes.io/aws-load-balancer-type: nlb
  #  nodeAffinity: high
  #  resources:
  #    limits:
  #      cpu: 2
  #      memory: 2Gi